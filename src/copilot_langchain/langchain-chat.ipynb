{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from typing import Any, List\n",
    "#from langchain import PromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from azure.ai.generative.index import get_langchain_retriever_from_index\n",
    "\n",
    "from langchain.retrievers import AzureCognitiveSearchRetriever\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set credential values needed to authenciate and authorize access to your Azure OpenAI instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_credentials():\n",
    "    # Azure OpenAI credentials\n",
    "    openai.api_type = os.environ[\"OPENAI_API_TYPE\"]\n",
    "    openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "    openai.api_version = os.environ[\"OPENAI_API_VERSION\"]\n",
    "    openai.api_base = os.environ[\"OPENAI_API_BASE\"]\n",
    "\n",
    "    # Azure Cognitive Search credentials\n",
    "    os.environ[\"AZURE_COGNITIVE_SEARCH_TARGET\"] = os.environ[\"AZURE_AI_SEARCH_ENDPOINT\"]\n",
    "    os.environ[\"AZURE_COGNITIVE_SEARCH_API_KEY\"] = os.environ[\"AZURE_AI_SEARCH_KEY\"]\n",
    "    os.environ[\"AZURE_COGNITIVE_SEARCH_SERVICE_NAME\"] = os.environ[\"AZURE_AI_SEARCH_KEY\"]\n",
    "    os.environ[\"AZURE_COGNITIVE_SEARCH_INDEX_NAME\"] = os.environ[\"AZURE_AI_SEARCH_INDEX_NAME\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the authorize with the given credentials above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_credentials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "def get_search_resource_name(srv_url):\n",
    "    val = urlparse(srv_url)\n",
    "    return val.netloc\n",
    "endpoint = get_search_resource_name(os.getenv('AZURE_AI_SEARCH_ENDPOINT'))\n",
    "resource_name =str(endpoint).split('.', 1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import AzureCognitiveSearchRetriever\n",
    "retriever = AzureCognitiveSearchRetriever(content_key=\"content\", top_k=10, service_name=resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set you cognitive search endpoint and key for the vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the AzureSearch instance with the OpenAIEmbeddings to vectorize the input sample text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct your prompt by specifying what the chat assistant does as well as the scope and domain of topics it can provide information for.  In addition, define any constraints, restricts or bounderies on how the prompt show behave.  The prompt template includes action the prompt can take; chat history of the system/user dialogue; context of the chat conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "System:\n",
    "You are an AI assistant helping users with queries related to outdoor outdooor/camping gear and clothing.\n",
    "Use the following pieces of context to answer the questions about outdoor/camping gear and clothing as completely, correctly, and concisely as possible.\n",
    "\n",
    "---\n",
    "\n",
    "{chat_history}\n",
    "\n",
    "------\n",
    "\n",
    "{context}\n",
    "\n",
    "------\n",
    "\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\n",
    "        \"context\",\n",
    "        \"chat_history\",\n",
    "        \"question\"\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the AzureChatOpenAI instance with your Azure Open AI model and deployment.  The temperature value ranges from 0 to 1.  Value close to 0 denote how specific you want the response to be and value close to 1 denote how randomly you want the responses to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT\"],\n",
    "    model_name=os.environ[\"AZURE_OPENAI_CHAT_MODEL\"],\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can ready to invoke the key of langchain you want to use.  <provide the type of conversion you want>.  To activate the instance you need your LLM model to retrieve response, the promt template including the rules, and verbose <???>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "qna_chain = ConversationalRetrievalChain.from_llm(llm=llm,\n",
    "                                           retriever=retriever,\n",
    "                                           condense_question_prompt=prompt_template,\n",
    "                                           #return_source_documents=True,\n",
    "                                           verbose=False,\n",
    "                                           memory=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the chat with a question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The CozyNights Sleeping Bag (item_number: 7) and the MountainDream Sleeping Bag (item_number: 14) are both made of polyester.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#when you want to initalize chat with history.\n",
    "\n",
    "inquiry = \"What of your sleeping bags are polyester?\"\n",
    "response = qna_chain({\"question\": inquiry})\n",
    "response['answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
