{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Quickstart Sample\n",
    "\n",
    "This project use the AI Search service to create a vector store for a custom department store data.  We’ll use Retrieval Augmented Generation (RAG), a pattern used in AI which uses Azure Open AI LLM to generate answers with your own data. In addition, we’ll construct prompt template to provide the scope of our dataset, as well as the context to the submit questions. Next, we’ll maintain the state of the QnA by storing the chat history in the prompt. Lastly, to enable the user to ask questions our data in a conversational format, we’ll using Langchain to connect our prompt template with our Azure Open AI LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from typing import Any, List\n",
    "#from langchain import PromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from azure.ai.generative.index import get_langchain_retriever_from_index\n",
    "\n",
    "from langchain.retrievers import AzureCognitiveSearchRetriever\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set credential values needed to authenciate and authorize access to your Azure OpenAI instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_credentials():\n",
    "    # Azure OpenAI credentials\n",
    "    openai.api_type = os.environ[\"OPENAI_API_TYPE\"]\n",
    "    openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "    openai.api_version = os.environ[\"OPENAI_API_VERSION\"]\n",
    "    openai.api_base = os.environ[\"OPENAI_API_BASE\"]\n",
    "\n",
    "    # Azure Cognitive Search credentials\n",
    "    os.environ[\"AZURE_COGNITIVE_SEARCH_TARGET\"] = os.environ[\"AZURE_AI_SEARCH_ENDPOINT\"]\n",
    "    os.environ[\"AZURE_COGNITIVE_SEARCH_API_KEY\"] = os.environ[\"AZURE_AI_SEARCH_KEY\"]\n",
    "    os.environ[\"AZURE_COGNITIVE_SEARCH_SERVICE_NAME\"] = os.environ[\"AZURE_AI_SEARCH_KEY\"]\n",
    "    os.environ[\"AZURE_COGNITIVE_SEARCH_INDEX_NAME\"] = os.environ[\"AZURE_AI_SEARCH_INDEX_NAME\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the authorize with the given credentials above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_credentials()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we're using a helper function to extract our AI Search name from the endpoint URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "def get_search_resource_name(srv_url):\n",
    "    val = urlparse(srv_url)\n",
    "    return val.netloc\n",
    "endpoint = get_search_resource_name(os.getenv('AZURE_AI_SEARCH_ENDPOINT'))\n",
    "resource_name =str(endpoint).split('.', 1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use LangChain's [AzureCognitiveSearchRetriever](https://api.python.langchain.com/en/latest/retrievers/langchain_community.retrievers.azure_cognitive_search.AzureCognitiveSearchRetriever.html#) library.  This is a useful retriever that seamlessly takes the user's query input and searches the vector database in [Azure AI Search](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search) (formerly known as \"Azure Cognitive Search\") with minimal code.  The retriever uses the MLIndex model to find the most relevant results from the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import AzureCognitiveSearchRetriever\n",
    "retriever = AzureCognitiveSearchRetriever(content_key=\"content\", top_k=10, service_name=resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct your prompt by specifying what the chat assistant does as well as the scope and domain of topics it can provide information for.  In addition, define any constraints, restrictions or bounderies on how the prompt should behave.  The prompt template includes action the prompt can take; chat history of the system/user dialogue; context of the chat conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "System:\n",
    "You are an AI assistant helping users with queries related to outdoor outdooor/camping gear and clothing.\n",
    "Use the following pieces of context to answer the questions about outdoor/camping gear and clothing as completely, correctly, and concisely as possible.\n",
    "\n",
    "---\n",
    "\n",
    "{chat_history}\n",
    "\n",
    "------\n",
    "\n",
    "{context}\n",
    "\n",
    "------\n",
    "\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\n",
    "        \"context\",\n",
    "        \"chat_history\",\n",
    "        \"question\"\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the AzureChatOpenAI instance with your Azure Open AI model (gpt-35-turbo) and deployment.  The temperature value ranges from 0 to 1.  Value closer to 0 denotes how specific you want the response to be; and a value closer to 1 denoter how random you want the responses to be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT\"],\n",
    "    model_name=os.environ[\"AZURE_OPENAI_CHAT_MODEL\"],\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConversationBufferMemory is a straigh-forward what for storing conversation history of a chat. The function takes the input and output of the chat and stores it in memory. The `chat_history` field from the prompt gets populated by memory.  The`memory_key` is where you specify which key to store in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you are ready to invoke the type of langchain you want to use.  **ConversationalRetrievalChain** connects the Azure Open AI LLM model, retriever, prompt template and chat memory in order to search the AI Search database to retrieve the most relevant response.  To activate the instance you need your LLM model to retrieve response, the promt template rules, and chat history.  Verbose is set to False to not display the prompt structure with the response values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "qna_chain = ConversationalRetrievalChain.from_llm(llm=llm,\n",
    "                                           retriever=retriever,\n",
    "                                           condense_question_prompt=prompt_template,\n",
    "                                           #return_source_documents=True,\n",
    "                                           verbose=False,\n",
    "                                           memory=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the chat with a question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The CozyNights Sleeping Bag (item_number: 7) and the MountainDream Sleeping Bag (item_number: 14) are both made of polyester.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Enter a question about outdoor gear and clothing\n",
    "inquiry = \"Which of your sleeping bags are polyester?\"\n",
    "response = qna_chain({\"question\": inquiry})\n",
    "response['answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
